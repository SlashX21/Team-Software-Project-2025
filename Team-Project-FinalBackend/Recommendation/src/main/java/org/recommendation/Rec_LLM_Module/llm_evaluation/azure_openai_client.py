"""
Azure OpenAIå®¢æˆ·ç«¯ - ç”Ÿäº§çº§å®ç°
æ”¯æŒå¤šç”¨æˆ·å¹¶å‘ã€é”™è¯¯æ¢å¤ã€æˆæœ¬è¿½è¸ªå’Œæ€§èƒ½ç›‘æ§
"""

import os
import time
import asyncio
import logging
from typing import Dict, List, Optional, Any
from openai import AzureOpenAI
from dataclasses import dataclass
from datetime import datetime

logger = logging.getLogger(__name__)

@dataclass
class AzureLLMResponse:
    """Azure LLMå“åº”æ•°æ®ç±»"""
    success: bool
    content: str
    usage: Dict[str, Any]
    model: str
    processing_time_ms: int
    attempt: int
    error: Optional[Dict] = None
    fallback_used: bool = False

class AzureOpenAIClient:
    """Azure OpenAIæœåŠ¡å®¢æˆ·ç«¯ - ç”Ÿäº§çº§å®ç°"""
    
    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–Azure OpenAIå®¢æˆ·ç«¯"""
        self.config = config or self._load_azure_config()
        
        # åˆå§‹åŒ–Azure OpenAIå®¢æˆ·ç«¯
        self.client = AzureOpenAI(
            api_key=self.config["api_key"],
            api_version=self.config["api_version"],
            azure_endpoint=self.config["endpoint"]
        )
        
        # æœåŠ¡é…ç½®
        self.deployment_name = self.config["model"]  # gpt-4o-mini-prod
        # Azure OpenAI o4-miniéœ€è¦æ›´å¤štokensç”¨äºæ¨ç† + è¾“å‡º - æµ‹è¯•é˜¶æ®µä½¿ç”¨æœ€å¤§çª—å£
        self.max_tokens = self.config.get("max_tokens", 4000)
        # self.temperature = self.config.get("temperature", 0.7)  # Removed for o4-mini compatibility
        self.timeout = self.config.get("timeout", 30)
        self.retry_attempts = self.config.get("retry_attempts", 3)
        
        # ä½¿ç”¨ç»Ÿè®¡
        self.usage_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens": 0,
            "total_cost_usd": 0.0,
            "average_response_time_ms": 0.0
        }
        
        self.response_times = []
        
    def _load_azure_config(self) -> Dict[str, str]:
        """åŠ è½½Azureé…ç½®"""
        api_key = os.getenv("AZURE_OPENAI_API_KEY", "a0aad09ad49949f8960ed30cc9d39c0a")
        endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "https://xiangopenai2025.openai.azure.com/")
        deployment_name = os.getenv("AZURE_OPENAI_MODEL", "o4-mini")
        
        # éªŒè¯å¿…éœ€çš„é…ç½®
        if not api_key or api_key == "":
            raise ValueError("Azure OpenAI API key is required but not configured")
        if not endpoint or endpoint == "":
            raise ValueError("Azure OpenAI endpoint is required but not configured")
        if not deployment_name or deployment_name == "":
            raise ValueError("Azure OpenAI deployment name is required but not configured")
            
        logger.info(f"Azure OpenAI configuration loaded: endpoint={endpoint}, deployment={deployment_name}")
        
        return {
            "api_key": api_key,
            "endpoint": endpoint,
            "api_version": os.getenv("AZURE_OPENAI_API_VERSION", "2024-12-01-preview"),
            "model": deployment_name,  # This is the deployment name for Azure
            "max_tokens": int(os.getenv("AZURE_MAX_TOKENS", "4000")),
            # "temperature": float(os.getenv("AZURE_TEMPERATURE", "0.7")),  # Removed for o4-mini compatibility
            "timeout": int(os.getenv("AZURE_TIMEOUT", "30")),
            "retry_attempts": int(os.getenv("AZURE_RETRY_ATTEMPTS", "3"))
        }
    
    async def generate_completion(self, prompt: str, 
                                config_override: Optional[Dict] = None) -> AzureLLMResponse:
        """
        ç”ŸæˆAIå›å¤ - å…¼å®¹ç°æœ‰æ¥å£
        """
        start_time = time.time()
        
        # åˆå¹¶é…ç½® - Azure OpenAI uses max_completion_tokens instead of max_tokens
        effective_config = {
            "max_tokens": self.max_tokens,
            **(config_override or {})
        }
        
        # âŒ AZURE o4-mini BUG FIX: ä¸è®¾ç½®max_completion_tokens
        # ç»æµ‹è¯•å‘ç°o4-miniæ¨¡å‹è®¾ç½®max_completion_tokensä¼šå¯¼è‡´è¿”å›ç©ºå†…å®¹
        # ç§»é™¤æ‰€æœ‰tokensé™åˆ¶å‚æ•°ï¼Œè®©æ¨¡å‹è‡ªç”±ç”Ÿæˆ
        effective_config.pop("max_tokens", None)
        effective_config.pop("max_completion_tokens", None)
        
        logger.info("ğŸ”§ Azure o4-miniä¿®å¤: ç§»é™¤max_completion_tokensé™åˆ¶")
        
        # Note: temperature removed for o4-mini compatibility
        
        # æ›´æ–°è¯·æ±‚ç»Ÿè®¡
        self.usage_stats["total_requests"] += 1
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(1, self.retry_attempts + 1):
            try:
                # è°ƒç”¨Azure OpenAI API
                response = await self._call_azure_api(prompt, effective_config)
                
                processing_time_ms = int((time.time() - start_time) * 1000)
                self.response_times.append(processing_time_ms)
                
                # æ›´æ–°æˆåŠŸç»Ÿè®¡
                self.usage_stats["successful_requests"] += 1
                self._update_usage_stats(response, processing_time_ms)
                
                return AzureLLMResponse(
                    success=True,
                    content=response.choices[0].message.content,
                    usage=self._extract_usage_info(response),
                    model=self.deployment_name,
                    processing_time_ms=processing_time_ms,
                    attempt=attempt,
                    fallback_used=False
                )
                
            except Exception as e:
                logger.warning(f"Azure OpenAIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt}/{self.retry_attempts}): {e}")
                
                if attempt == self.retry_attempts:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                    processing_time_ms = int((time.time() - start_time) * 1000)
                    self.usage_stats["failed_requests"] += 1
                    
                    return AzureLLMResponse(
                        success=False,
                        content="",
                        usage={},
                        model=self.deployment_name,
                        processing_time_ms=processing_time_ms,
                        attempt=attempt,
                        error=self._format_azure_error(e),
                        fallback_used=False
                    )
                
                # ç­‰å¾…åé‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
                await asyncio.sleep(min(2 ** (attempt - 1), 10))
    
    async def _call_azure_api(self, prompt: str, config: Dict) -> Any:
        """è°ƒç”¨Azure OpenAI API"""
        try:
            # âŒ AZURE o4-mini BUG FIX: å®Œå…¨ç§»é™¤max_completion_tokenså‚æ•°
            # ç»æµ‹è¯•éªŒè¯ï¼Œo4-miniæ¨¡å‹è®¾ç½®max_completion_tokensä¼šè¿”å›ç©ºå†…å®¹
            logger.debug(f"Azure API call: deployment={self.deployment_name} (æ— tokensé™åˆ¶)")
            
            # æ„å»ºAPIè°ƒç”¨å‚æ•°ï¼Œä¸åŒ…å«ä»»ä½•tokensé™åˆ¶
            api_params = {
                "model": self.deployment_name,
                "messages": [
                    {"role": "system", "content": "You are a professional nutritionist and food safety expert."},
                    {"role": "user", "content": prompt}
                ]
                # âŒ ç§»é™¤max_completion_tokenså‚æ•°
                # âŒ ç§»é™¤temperatureå‚æ•° (o4-miniåªæ”¯æŒé»˜è®¤å€¼1)
            }
            
            logger.info(f"ğŸ”§ Azure APIå‚æ•°: {list(api_params.keys())}")
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    self.client.chat.completions.create,
                    **api_params
                ),
                timeout=self.timeout
            )
            # Detailed response logging for debugging
            if response.choices:
                content = response.choices[0].message.content
                logger.info(f"Azure API response details: choices_count={len(response.choices)}, content_length={len(content) if content else 0}")
                logger.info(f"Azure API response content preview: '{content[:100] if content else 'EMPTY'}...'")
            else:
                logger.warning("Azure API response has no choices")
            
            return response
        except asyncio.TimeoutError:
            logger.error(f"Azure OpenAI APIè°ƒç”¨è¶…æ—¶ ({self.timeout}s) for deployment {self.deployment_name}")
            raise Exception(f"Azure request timeout after {self.timeout} seconds for deployment {self.deployment_name}")
        except Exception as e:
            error_str = str(e)
            logger.error(f"Azure OpenAI APIè°ƒç”¨å¤±è´¥: {error_str} (deployment: {self.deployment_name})")
            
            # å¢å¼ºé”™è¯¯ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
            if "401" in error_str or "invalid api key" in error_str.lower():
                raise Exception(f"Azure authentication failed: Invalid API key for deployment {self.deployment_name}")
            elif "404" in error_str or "deployment" in error_str.lower():
                raise Exception(f"Azure deployment not found: {self.deployment_name} not available at {self.config['endpoint']}")
            elif "429" in error_str or "rate limit" in error_str.lower():
                raise Exception(f"Azure rate limit exceeded for deployment {self.deployment_name}")
            else:
                raise Exception(f"Azure API error for deployment {self.deployment_name}: {error_str}")
    
    def _extract_usage_info(self, response: Any) -> Dict[str, Any]:
        """æå–ä½¿ç”¨ä¿¡æ¯"""
        try:
            if hasattr(response, 'usage'):
                usage = response.usage
                return {
                    "total_tokens": getattr(usage, 'total_tokens', 0),
                    "prompt_tokens": getattr(usage, 'prompt_tokens', 0),
                    "completion_tokens": getattr(usage, 'completion_tokens', 0)
                }
        except Exception as e:
            logger.warning(f"æå–Azure usageä¿¡æ¯å¤±è´¥: {e}")
        return {"total_tokens": 0, "estimated": True}
    
    def _update_usage_stats(self, response: Any, processing_time_ms: int):
        """æ›´æ–°ä½¿ç”¨ç»Ÿè®¡"""
        usage_info = self._extract_usage_info(response)
        tokens_used = usage_info.get("total_tokens", 0)
        
        self.usage_stats["total_tokens"] += tokens_used
        
        # ä¼°ç®—æˆæœ¬ï¼ˆåŸºäºAzure OpenAIå®šä»·ï¼‰
        estimated_cost = self._estimate_cost(usage_info)
        self.usage_stats["total_cost_usd"] += estimated_cost
        
        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        if self.response_times:
            self.usage_stats["average_response_time_ms"] = sum(self.response_times) / len(self.response_times)
    
    def _estimate_cost(self, usage_info: Dict) -> float:
        """ä¼°ç®—APIè°ƒç”¨æˆæœ¬"""
        # Azure OpenAI gpt-4o-miniå®šä»·ï¼ˆä¼°ç®—ï¼‰
        input_cost_per_1k = 0.00015  # $0.15/1K tokens
        output_cost_per_1k = 0.0006   # $0.60/1K tokens
        
        prompt_tokens = usage_info.get("prompt_tokens", 0)
        completion_tokens = usage_info.get("completion_tokens", 0)
        
        input_cost = (prompt_tokens / 1000) * input_cost_per_1k
        output_cost = (completion_tokens / 1000) * output_cost_per_1k
        
        return input_cost + output_cost
    
    def _format_azure_error(self, error: Exception) -> Dict[str, Any]:
        """æ ¼å¼åŒ–Azureé”™è¯¯ä¿¡æ¯"""
        error_message = str(error).lower()
        
        if "rate limit" in error_message or "429" in error_message:
            error_code = "AZURE_RATE_LIMIT_EXCEEDED"
            user_message = "AzureæœåŠ¡ç¹å¿™ï¼Œè¯·ç¨åé‡è¯•"
        elif "invalid api key" in error_message or "401" in error_message:
            error_code = "AZURE_AUTH_ERROR"
            user_message = "AzureæœåŠ¡è®¤è¯å¤±è´¥"
        elif "timeout" in error_message:
            error_code = "AZURE_TIMEOUT"
            user_message = "AzureæœåŠ¡å“åº”è¶…æ—¶"
        else:
            error_code = "AZURE_API_ERROR"
            user_message = "Azure AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
        
        return {
            "code": error_code,
            "message": user_message,
            "details": {
                "original_error": str(error),
                "timestamp": datetime.now().isoformat(),
                "deployment": self.deployment_name
            }
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """AzureæœåŠ¡å¥åº·æ£€æŸ¥"""
        try:
            test_prompt = "è¯·å›å¤'Azureå¥åº·æ£€æŸ¥é€šè¿‡'"
            start_time = time.time()
            
            response = await self._call_azure_api(test_prompt, {"max_tokens": 50})
            response_time = (time.time() - start_time) * 1000
            
            return {
                "status": "healthy",
                "service": "Azure OpenAI",
                "deployment": self.deployment_name,
                "endpoint": self.config["endpoint"],
                "response_time_ms": response_time,
                "api_accessible": True,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "service": "Azure OpenAI", 
                "deployment": self.deployment_name,
                "endpoint": self.config["endpoint"],
                "error": str(e),
                "api_accessible": False,
                "timestamp": datetime.now().isoformat()
            }
    
    def get_usage_statistics(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.usage_stats.copy()
        
        # è®¡ç®—æˆåŠŸç‡
        total_requests = stats["total_requests"]
        if total_requests > 0:
            stats["success_rate"] = stats["successful_requests"] / total_requests
            stats["failure_rate"] = stats["failed_requests"] / total_requests
        else:
            stats["success_rate"] = 0.0
            stats["failure_rate"] = 0.0
        
        # æ·»åŠ å“åº”æ—¶é—´ç»Ÿè®¡
        if self.response_times:
            stats["response_time_stats"] = {
                "min_ms": min(self.response_times),
                "max_ms": max(self.response_times),
                "avg_ms": stats["average_response_time_ms"],
                "p95_ms": self._calculate_percentile(self.response_times, 0.95),
                "p99_ms": self._calculate_percentile(self.response_times, 0.99)
            }
        
        stats["generated_at"] = datetime.now().isoformat()
        return stats
    
    def _calculate_percentile(self, values: List[float], percentile: float) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not values:
            return 0.0
        
        sorted_values = sorted(values)
        index = int(len(sorted_values) * percentile)
        return sorted_values[min(index, len(sorted_values) - 1)]
    
    # å…¼å®¹ç°æœ‰æ¥å£
    def generate_completion_sync(self, prompt: str, 
                               config_override: Optional[Dict] = None) -> AzureLLMResponse:
        """åŒæ­¥ç‰ˆæœ¬çš„completionç”Ÿæˆ"""
        return asyncio.run(self.generate_completion(prompt, config_override))

# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
_global_azure_client = None

def get_azure_openai_client() -> AzureOpenAIClient:
    """è·å–å…¨å±€Azure OpenAIå®¢æˆ·ç«¯å®ä¾‹"""
    global _global_azure_client
    if _global_azure_client is None:
        _global_azure_client = AzureOpenAIClient()
    return _global_azure_client

# ä¾¿æ·å‡½æ•°
async def generate_azure_ai_analysis(prompt: str, **kwargs) -> str:
    """ä¾¿æ·çš„Azure AIåˆ†æç”Ÿæˆå‡½æ•°"""
    client = get_azure_openai_client()
    response = await client.generate_completion(prompt, kwargs)
    
    if response.success:
        return response.content
    else:
        logger.error(f"Azure AIåˆ†æç”Ÿæˆå¤±è´¥: {response.error}")
        return "Azure AIåˆ†ææš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚"

def generate_azure_ai_analysis_sync(prompt: str, **kwargs) -> str:
    """åŒæ­¥ç‰ˆæœ¬çš„Azure AIåˆ†æç”Ÿæˆ"""
    return asyncio.run(generate_azure_ai_analysis(prompt, **kwargs))