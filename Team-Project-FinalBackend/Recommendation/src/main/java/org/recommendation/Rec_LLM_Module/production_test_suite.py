#!/usr/bin/env python3
"""
Grocery Guardian Recommendationæ¨¡å—ç”Ÿäº§ç¯å¢ƒæµ‹è¯•å¥—ä»¶
æ•´åˆæ‰€æœ‰ç”Ÿäº§ä¼˜åŒ–åŠŸèƒ½çš„å®Œæ•´æµ‹è¯•ï¼ŒåŒ…å«è¯¦ç»†çš„ä¸­æ–‡æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ

æµ‹è¯•èŒƒå›´ï¼š
- Azure OpenAIé›†æˆæµ‹è¯•
- å¤šç”¨æˆ·å¹¶å‘å¤„ç†æµ‹è¯•
- æ•°æ®éªŒè¯å’Œå®‰å…¨æµ‹è¯•
- é”™è¯¯å¤„ç†å’Œé™çº§æœåŠ¡æµ‹è¯•
- æ€§èƒ½ç›‘æ§å’Œå¥åº·æ£€æŸ¥æµ‹è¯•
- ç«¯åˆ°ç«¯é›†æˆå·¥ä½œæµç¨‹æµ‹è¯•
"""

import asyncio
import sys
import os
import time
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TestResult:
    """æµ‹è¯•ç»“æœç±»"""
    def __init__(self, test_name: str, category: str):
        self.test_name = test_name
        self.category = category
        self.success = False
        self.start_time = time.time()
        self.end_time = None
        self.duration_ms = 0
        self.details = {}
        self.error_message = ""
        self.metrics = {}
    
    def complete(self, success: bool, details: Dict[str, Any] = None, error: str = ""):
        """å®Œæˆæµ‹è¯•"""
        self.end_time = time.time()
        self.duration_ms = int((self.end_time - self.start_time) * 1000)
        self.success = success
        self.details = details or {}
        self.error_message = error

class ProductionTestSuite:
    """ç”Ÿäº§ç¯å¢ƒæµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.test_results = []
        self.start_time = time.time()
        self.environment_info = self._collect_environment_info()
        self.test_summary = {
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "categories": {},
            "performance_metrics": {},
            "system_status": "unknown"
        }
    
    def _collect_environment_info(self) -> Dict[str, Any]:
        """æ”¶é›†ç¯å¢ƒä¿¡æ¯"""
        return {
            "test_start_time": datetime.now().isoformat(),
            "python_version": sys.version,
            "azure_openai_configured": bool(os.getenv("AZURE_OPENAI_API_KEY")),
            "openai_fallback_configured": bool(os.getenv("OPENAI_API_KEY")),
            "environment": os.getenv("ENVIRONMENT", "unknown"),
            "test_runner": "ProductionTestSuite v1.0"
        }
    
    async def run_azure_openai_integration_test(self) -> TestResult:
        """æµ‹è¯•Azure OpenAIé›†æˆåŠŸèƒ½"""
        result = TestResult("Azure OpenAIé›†æˆæµ‹è¯•", "AIæœåŠ¡")
        
        try:
            from llm_evaluation.azure_openai_client import AzureOpenAIClient
            
            print("ğŸ” æµ‹è¯•Azure OpenAIé›†æˆ...")
            
            # åˆ›å»ºAzureå®¢æˆ·ç«¯
            client = AzureOpenAIClient()
            
            # æµ‹è¯•åŸºç¡€è¯·æ±‚
            start_time = time.time()
            response = await client.generate_completion(
                "è¯·ç”¨ä¸€å¥è¯ç®€å•ä»‹ç»å¥åº·é¥®é£Ÿçš„é‡è¦æ€§ã€‚",
                config_override={"max_tokens": 100}
            )
            api_response_time = int((time.time() - start_time) * 1000)
            
            if response.success:
                # è·å–ç»Ÿè®¡ä¿¡æ¯
                stats = client.get_usage_statistics()
                
                result.complete(True, {
                    "api_response_time_ms": api_response_time,
                    "model": response.model,
                    "content_length": len(response.content),
                    "token_usage": response.usage.get('total_tokens', 0),
                    "prompt_tokens": response.usage.get('prompt_tokens', 0),
                    "completion_tokens": response.usage.get('completion_tokens', 0),
                    "total_requests": stats['total_requests'],
                    "success_rate": stats['success_rate'],
                    "response_preview": response.content[:100] + "..." if len(response.content) > 100 else response.content
                })
                print(f"âœ… Azure OpenAIé›†æˆæˆåŠŸ - å“åº”æ—¶é—´: {api_response_time}ms")
            else:
                result.complete(False, {"error_details": response.error}, str(response.error))
                print(f"âŒ Azure OpenAIè¯·æ±‚å¤±è´¥: {response.error}")
                
        except Exception as e:
            result.complete(False, {"exception_type": type(e).__name__}, str(e))
            print(f"âŒ Azure OpenAIé›†æˆæµ‹è¯•å¼‚å¸¸: {e}")
        
        return result
    
    async def run_concurrent_processing_test(self) -> TestResult:
        """æµ‹è¯•å¤šç”¨æˆ·å¹¶å‘å¤„ç†èƒ½åŠ›"""
        result = TestResult("å¹¶å‘å¤„ç†èƒ½åŠ›æµ‹è¯•", "æ€§èƒ½")
        
        try:
            from common.request_queue import RequestQueueManager, RequestPriority
            
            print("ğŸ” æµ‹è¯•å¤šç”¨æˆ·å¹¶å‘å¤„ç†...")
            
            # åˆ›å»ºé˜Ÿåˆ—ç®¡ç†å™¨
            queue_manager = RequestQueueManager(max_concurrent=5, max_queue_size=20)
            
            # æ¨¡æ‹Ÿå¤„ç†å‡½æ•°
            processing_times = []
            async def concurrent_processor(data):
                start = time.time()
                await asyncio.sleep(0.05)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                process_time = int((time.time() - start) * 1000)
                processing_times.append(process_time)
                return {"result": f"å¤„ç†å®Œæˆ: {data.get('request_id')}", "process_time": process_time}
            
            # æ³¨å†Œå¤„ç†å™¨å¹¶å¯åŠ¨
            queue_manager.register_processor("concurrent_test", concurrent_processor)
            await queue_manager.start()
            
            # å¹¶å‘æäº¤å¤šä¸ªç”¨æˆ·çš„è¯·æ±‚
            request_ids = []
            users = [1, 2, 3, 4, 5]  # 5ä¸ªä¸åŒç”¨æˆ·
            
            start_time = time.time()
            for user_id in users:
                for i in range(3):  # æ¯ä¸ªç”¨æˆ·3ä¸ªè¯·æ±‚
                    request_id = await queue_manager.enqueue_request(
                        user_id=user_id,
                        request_type="concurrent_test",
                        request_data={"request_id": f"user_{user_id}_req_{i}", "user_id": user_id},
                        priority=RequestPriority.NORMAL
                    )
                    request_ids.append((request_id, user_id))
            
            # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å¤„ç†å®Œæˆ
            await asyncio.sleep(2)
            total_time = int((time.time() - start_time) * 1000)
            
            # æ£€æŸ¥ç»“æœ
            completed_count = 0
            successful_count = 0
            for request_id, user_id in request_ids:
                status = await queue_manager.get_request_status(request_id)
                if status:
                    completed_count += 1
                    if status["status"] == "completed":
                        successful_count += 1
            
            # è·å–é˜Ÿåˆ—ç»Ÿè®¡
            stats = queue_manager.get_queue_stats()
            await queue_manager.stop()
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
            throughput = len(request_ids) / (total_time / 1000) if total_time > 0 else 0
            
            success = successful_count == len(request_ids)
            result.complete(success, {
                "total_requests": len(request_ids),
                "completed_requests": completed_count,
                "successful_requests": successful_count,
                "total_processing_time_ms": total_time,
                "avg_processing_time_ms": int(avg_processing_time),
                "throughput_requests_per_second": round(throughput, 2),
                "unique_users": len(users),
                "requests_per_user": 3,
                "queue_stats": stats
            })
            
            if success:
                print(f"âœ… å¹¶å‘å¤„ç†æµ‹è¯•é€šè¿‡ - å¤„ç†{len(request_ids)}ä¸ªè¯·æ±‚ï¼Œååé‡: {throughput:.2f} req/s")
            else:
                print(f"âŒ å¹¶å‘å¤„ç†æµ‹è¯•å¤±è´¥ - å®Œæˆ: {successful_count}/{len(request_ids)}")
                
        except Exception as e:
            result.complete(False, {"exception_type": type(e).__name__}, str(e))
            print(f"âŒ å¹¶å‘å¤„ç†æµ‹è¯•å¼‚å¸¸: {e}")
        
        return result
    
    async def run_data_validation_security_test(self) -> TestResult:
        """æµ‹è¯•æ•°æ®éªŒè¯å’Œå®‰å…¨åŠŸèƒ½"""
        result = TestResult("æ•°æ®éªŒè¯å’Œå®‰å…¨æµ‹è¯•", "å®‰å…¨")
        
        try:
            from common.data_validator import validate_recommendation_request
            
            print("ğŸ” æµ‹è¯•æ•°æ®éªŒè¯å’Œå®‰å…¨...")
            
            test_cases = [
                {
                    "name": "æœ‰æ•ˆæ¡ç æ¨èè¯·æ±‚",
                    "type": "barcode_recommendation",
                    "data": {
                        "userId": 1,
                        "productBarcode": "1234567890123",
                        "userPreferences": {
                            "healthGoal": "lose_weight",
                            "allergens": ["nuts", "dairy"],
                            "dietaryRestrictions": ["vegetarian"]
                        }
                    },
                    "should_pass": True
                },
                {
                    "name": "æœ‰æ•ˆå°ç¥¨åˆ†æè¯·æ±‚",
                    "type": "receipt_analysis",
                    "data": {
                        "userId": 2,
                        "purchasedItems": [
                            {"barcode": "1234567890123", "quantity": 2, "price": 5.99},
                            {"barcode": "9876543210987", "quantity": 1, "price": 3.50}
                        ],
                        "receiptInfo": {
                            "storeName": "æµ‹è¯•è¶…å¸‚",
                            "totalAmount": 15.48,
                            "purchaseDate": "2025-01-30T10:30:00Z"
                        }
                    },
                    "should_pass": True
                },
                {
                    "name": "æ— æ•ˆç”¨æˆ·ID",
                    "type": "barcode_recommendation",
                    "data": {"userId": -1, "productBarcode": "1234567890123"},
                    "should_pass": False
                },
                {
                    "name": "æ— æ•ˆæ¡ç æ ¼å¼",
                    "type": "barcode_recommendation", 
                    "data": {"userId": 1, "productBarcode": "invalid_barcode"},
                    "should_pass": False
                },
                {
                    "name": "æ½œåœ¨æ¶æ„è¾“å…¥",
                    "type": "barcode_recommendation",
                    "data": {
                        "userId": 1,
                        "productBarcode": "1234567890123",
                        "userPreferences": {
                            "healthGoal": "<script>alert('xss')</script>",
                            "allergens": ["'; DROP TABLE users; --"]
                        }
                    },
                    "should_pass": False
                }
            ]
            
            passed_tests = 0
            security_blocks = 0
            validation_details = []
            
            for test_case in test_cases:
                test_result = validate_recommendation_request(test_case["type"], test_case["data"])
                
                if test_case["should_pass"]:
                    if test_result.is_valid:
                        passed_tests += 1
                        validation_details.append({
                            "test": test_case["name"],
                            "status": "é€šè¿‡",
                            "result": "æœ‰æ•ˆè¯·æ±‚æ­£ç¡®éªŒè¯"
                        })
                    else:
                        validation_details.append({
                            "test": test_case["name"],
                            "status": "å¤±è´¥",
                            "result": f"æœ‰æ•ˆè¯·æ±‚è¢«é”™è¯¯æ‹’ç»: {test_result.errors}"
                        })
                else:
                    if not test_result.is_valid:
                        passed_tests += 1
                        security_blocks += 1
                        validation_details.append({
                            "test": test_case["name"],
                            "status": "é€šè¿‡",
                            "result": f"æ— æ•ˆ/æ¶æ„è¯·æ±‚æ­£ç¡®æ‹’ç»: {len(test_result.errors)}ä¸ªé”™è¯¯"
                        })
                    else:
                        validation_details.append({
                            "test": test_case["name"],
                            "status": "å¤±è´¥",
                            "result": "å±é™©è¯·æ±‚æœªè¢«æ‹¦æˆª"
                        })
            
            success = passed_tests == len(test_cases)
            result.complete(success, {
                "total_test_cases": len(test_cases),
                "passed_tests": passed_tests,
                "security_blocks": security_blocks,
                "validation_details": validation_details,
                "security_coverage": f"{security_blocks}/{len([t for t in test_cases if not t['should_pass']])}"
            })
            
            if success:
                print(f"âœ… æ•°æ®éªŒè¯æµ‹è¯•é€šè¿‡ - {passed_tests}/{len(test_cases)}ä¸ªæµ‹è¯•ç”¨ä¾‹é€šè¿‡")
            else:
                print(f"âŒ æ•°æ®éªŒè¯æµ‹è¯•å¤±è´¥ - {passed_tests}/{len(test_cases)}ä¸ªæµ‹è¯•ç”¨ä¾‹é€šè¿‡")
                
        except Exception as e:
            result.complete(False, {"exception_type": type(e).__name__}, str(e))
            print(f"âŒ æ•°æ®éªŒè¯æµ‹è¯•å¼‚å¸¸: {e}")
        
        return result
    
    async def run_error_recovery_test(self) -> TestResult:
        """æµ‹è¯•é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶"""
        result = TestResult("é”™è¯¯å¤„ç†å’Œæ¢å¤æµ‹è¯•", "å¯é æ€§")
        
        try:
            from common.error_handler import get_error_handler, ErrorContext
            from common.fallback_service import get_fallback_service
            
            print("ğŸ” æµ‹è¯•é”™è¯¯å¤„ç†å’Œæ¢å¤...")
            
            error_handler = get_error_handler()
            fallback_service = get_fallback_service()
            
            # æµ‹è¯•ä¸åŒç±»å‹çš„é”™è¯¯å¤„ç†
            error_scenarios = [
                {
                    "name": "AIæœåŠ¡é€Ÿç‡é™åˆ¶é”™è¯¯",
                    "error": Exception("Rate limit exceeded (429)"),
                    "context": ErrorContext(user_id=1, operation="ai_completion"),
                    "expected_category": "rate_limit_error"
                },
                {
                    "name": "ç½‘ç»œè¿æ¥é”™è¯¯",
                    "error": Exception("Connection timeout"),
                    "context": ErrorContext(user_id=2, operation="api_request"),
                    "expected_category": "network_error"
                },
                {
                    "name": "æ•°æ®åº“è¿æ¥é”™è¯¯",
                    "error": Exception("Database connection failed"),
                    "context": ErrorContext(user_id=3, operation="database_query"),
                    "expected_category": "database_error"
                }
            ]
            
            error_handling_results = []
            fallback_tests = []
            
            for scenario in error_scenarios:
                # æµ‹è¯•é”™è¯¯å¤„ç†
                error_info = error_handler.handle_error(scenario["error"], scenario["context"])
                
                error_handling_results.append({
                    "scenario": scenario["name"],
                    "error_code": error_info.code,
                    "category": error_info.category.value,
                    "severity": error_info.severity.value,
                    "fallback_available": error_info.fallback_available,
                    "retry_after": error_info.retry_after,
                    "user_message": error_info.user_message[:50] + "..." if len(error_info.user_message) > 50 else error_info.user_message
                })
                
                # å¦‚æœæ”¯æŒé™çº§ï¼Œæµ‹è¯•é™çº§æœåŠ¡
                if error_info.fallback_available:
                    fallback_result = fallback_service.get_barcode_fallback_recommendation(
                        user_id=scenario["context"].user_id,
                        barcode="1234567890123"
                    )
                    
                    fallback_tests.append({
                        "scenario": scenario["name"],
                        "fallback_success": fallback_result["success"],
                        "fallback_mode": fallback_result["data"]["fallbackMode"],
                        "recommendations_count": len(fallback_result["data"]["recommendations"]),
                        "has_analysis": "llmAnalysis" in fallback_result["data"]
                    })
            
            # è·å–é”™è¯¯ç»Ÿè®¡
            error_stats = error_handler.get_error_statistics()
            
            success = all(r["error_code"] for r in error_handling_results) and all(f["fallback_success"] for f in fallback_tests)
            
            result.complete(success, {
                "error_scenarios_tested": len(error_scenarios),
                "error_handling_results": error_handling_results,
                "fallback_tests": fallback_tests,
                "error_statistics": error_stats,
                "successful_fallbacks": len([f for f in fallback_tests if f["fallback_success"]])
            })
            
            if success:
                print(f"âœ… é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡ - å¤„ç†{len(error_scenarios)}ç§é”™è¯¯åœºæ™¯")
            else:
                print(f"âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥")
                
        except Exception as e:
            result.complete(False, {"exception_type": type(e).__name__}, str(e))
            print(f"âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¼‚å¸¸: {e}")
        
        return result
    
    async def run_performance_monitoring_test(self) -> TestResult:
        """æµ‹è¯•æ€§èƒ½ç›‘æ§å’Œå¥åº·æ£€æŸ¥"""
        result = TestResult("æ€§èƒ½ç›‘æ§å’Œå¥åº·æ£€æŸ¥æµ‹è¯•", "ç›‘æ§")
        
        try:
            from monitoring.performance_monitor import get_performance_monitor
            from monitoring.health_checker import get_health_checker
            
            print("ğŸ” æµ‹è¯•æ€§èƒ½ç›‘æ§å’Œå¥åº·æ£€æŸ¥...")
            
            # æ€§èƒ½ç›‘æ§æµ‹è¯•
            monitor = get_performance_monitor()
            
            # æ¨¡æ‹Ÿä¸€äº›è¯·æ±‚æŒ‡æ ‡
            test_metrics = []
            for i in range(10):
                response_time = 100 + (i * 10)  # 100-190ms
                success = i < 8  # 80%æˆåŠŸç‡
                monitor.record_request(f"test_request", response_time, success, user_id=(i % 3) + 1)
                test_metrics.append({"response_time": response_time, "success": success})
            
            # æ¨¡æ‹ŸAIè¯·æ±‚æŒ‡æ ‡
            for i in range(5):
                ai_response_time = 200 + (i * 20)
                ai_success = i < 4  # 80%æˆåŠŸç‡
                monitor.record_ai_request(ai_response_time, ai_success, token_usage=50+i*10, cost_usd=0.001*(i+1))
            
            # æ›´æ–°ç³»ç»ŸæŒ‡æ ‡
            monitor.update_concurrent_requests(5)
            monitor.update_queue_size(8)
            monitor.update_user_metrics(active_users=3, total_users=5, sessions=3)
            
            # è·å–æ€§èƒ½æŠ¥å‘Š
            performance_report = monitor.get_performance_report()
            real_time_metrics = monitor.get_real_time_metrics()
            
            # å¥åº·æ£€æŸ¥æµ‹è¯•
            health_checker = get_health_checker()
            
            # æ³¨å†Œæµ‹è¯•å¥åº·æ£€æŸ¥
            async def test_healthy_service():
                return {"status": "healthy", "message": "æµ‹è¯•æœåŠ¡æ­£å¸¸", "response_time": 50}
            
            async def test_degraded_service():
                return {"status": "degraded", "message": "æµ‹è¯•æœåŠ¡æ€§èƒ½ä¸‹é™", "response_time": 200}
            
            health_checker.register_health_check("test_healthy", test_healthy_service)
            health_checker.register_health_check("test_degraded", test_degraded_service)
            
            # æ‰§è¡Œå¥åº·æ£€æŸ¥
            health_results = await health_checker.check_all_services()
            health_summary = health_checker.get_health_summary()
            
            # éªŒè¯ç»“æœ
            performance_valid = (
                performance_report["request_metrics"]["total_requests"] == 10 and
                performance_report["ai_metrics"]["total_ai_requests"] == 5 and
                performance_report["user_metrics"]["active_users"] == 3
            )
            
            health_valid = (
                len(health_results) >= 2 and
                health_summary["total_services"] >= 2
            )
            
            success = performance_valid and health_valid
            
            result.complete(success, {
                "performance_metrics": {
                    "total_requests": performance_report["request_metrics"]["total_requests"],
                    "success_rate": performance_report["request_metrics"]["success_rate"],
                    "ai_requests": performance_report["ai_metrics"]["total_ai_requests"],
                    "total_tokens": performance_report["ai_metrics"]["total_tokens"],
                    "total_cost": performance_report["ai_metrics"]["total_cost_usd"],
                    "active_users": performance_report["user_metrics"]["active_users"],
                    "concurrent_requests": real_time_metrics["current_concurrent_requests"],
                    "queue_size": real_time_metrics["current_queue_size"]
                },
                "health_check_results": {
                    "total_services": health_summary["total_services"],
                    "overall_status": health_summary["overall_status"],
                    "healthy_services": health_summary["status_distribution"].get("healthy", 0),
                    "degraded_services": health_summary["status_distribution"].get("degraded", 0),
                    "unhealthy_services": health_summary["status_distribution"].get("unhealthy", 0)
                },
                "monitoring_capabilities": {
                    "real_time_metrics": True,
                    "historical_data": True,
                    "health_monitoring": True,
                    "alert_system": performance_report.get("alerts", {}).get("active_alerts", 0) >= 0
                }
            })
            
            if success:
                print(f"âœ… ç›‘æ§ç³»ç»Ÿæµ‹è¯•é€šè¿‡ - æ€§èƒ½æŒ‡æ ‡å’Œå¥åº·æ£€æŸ¥æ­£å¸¸")
            else:
                print(f"âŒ ç›‘æ§ç³»ç»Ÿæµ‹è¯•å¤±è´¥")
                
        except Exception as e:
            result.complete(False, {"exception_type": type(e).__name__}, str(e))
            print(f"âŒ ç›‘æ§ç³»ç»Ÿæµ‹è¯•å¼‚å¸¸: {e}")
        
        return result
    
    async def run_end_to_end_workflow_test(self) -> TestResult:
        """æµ‹è¯•ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹"""
        result = TestResult("ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹æµ‹è¯•", "é›†æˆ")
        
        try:
            print("ğŸ” æµ‹è¯•ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹...")
            
            # å¯¼å…¥æ‰€éœ€æ¨¡å—
            from common.session_manager import get_session_manager
            from common.data_validator import validate_recommendation_request
            from common.request_queue import RequestQueueManager, RequestPriority
            from monitoring.performance_monitor import get_performance_monitor
            
            workflow_steps = []
            
            # æ­¥éª¤1: åˆ›å»ºç”¨æˆ·ä¼šè¯
            session_manager = get_session_manager()
            user_id = 12345
            session_id = session_manager.create_session(
                user_id=user_id,
                user_agent="TestAgent/1.0",
                ip_address="127.0.0.1"
            )
            workflow_steps.append({"step": "åˆ›å»ºç”¨æˆ·ä¼šè¯", "status": "æˆåŠŸ", "session_id": session_id})
            
            # æ­¥éª¤2: éªŒè¯è¯·æ±‚æ•°æ®
            request_data = {
                "userId": user_id,
                "productBarcode": "1234567890123",
                "userPreferences": {
                    "healthGoal": "lose_weight",
                    "allergens": ["nuts"],
                    "dietaryRestrictions": ["vegetarian"]
                }
            }
            
            validation_result = validate_recommendation_request("barcode_recommendation", request_data)
            if validation_result.is_valid:
                workflow_steps.append({"step": "æ•°æ®éªŒè¯", "status": "é€šè¿‡", "cleaned_data_keys": list(validation_result.cleaned_data.keys())})
            else:
                workflow_steps.append({"step": "æ•°æ®éªŒè¯", "status": "å¤±è´¥", "errors": validation_result.errors})
                raise Exception("æ•°æ®éªŒè¯å¤±è´¥")
            
            # æ­¥éª¤3: æ£€æŸ¥é‡å¤è¯·æ±‚
            is_duplicate = session_manager.check_duplicate_request(user_id, request_data)
            workflow_steps.append({"step": "é‡å¤è¯·æ±‚æ£€æŸ¥", "status": "é€šè¿‡" if not is_duplicate else "å‘ç°é‡å¤", "is_duplicate": is_duplicate})
            
            # æ­¥éª¤4: é˜Ÿåˆ—å¤„ç†
            queue_manager = RequestQueueManager(max_concurrent=3, max_queue_size=10)
            
            async def mock_recommendation_processor(data):
                # æ¨¡æ‹ŸAIæœåŠ¡è°ƒç”¨
                await asyncio.sleep(0.1)
                
                # è®°å½•AIæŒ‡æ ‡
                monitor = get_performance_monitor()
                monitor.record_ai_request(100, True, token_usage=75, cost_usd=0.0008)
                
                return {
                    "recommendationId": f"rec_{int(time.time())}",
                    "recommendations": [
                        {"productName": "ä½å¡è·¯é‡Œç‡•éº¦", "reason": "ç¬¦åˆå‡é‡ç›®æ ‡ï¼Œæ— åšæœæˆåˆ†"},
                        {"productName": "ç´ é£Ÿè›‹ç™½æ£’", "reason": "ç´ é£Ÿå‹å¥½ï¼Œé«˜è›‹ç™½"}
                    ],
                    "llmAnalysis": {
                        "summary": "åŸºäºæ‚¨çš„å‡é‡ç›®æ ‡å’Œç´ é£Ÿåå¥½ï¼Œæ¨èä½çƒ­é‡ã€é«˜è¥å…»å¯†åº¦çš„é£Ÿå“",
                        "nutritionScore": 85,
                        "healthScore": 90
                    }
                }
            
            queue_manager.register_processor("barcode_recommendation", mock_recommendation_processor)
            await queue_manager.start()
            
            # æ­¥éª¤5: æäº¤è¯·æ±‚åˆ°é˜Ÿåˆ—
            start_time = time.time()
            request_id = await queue_manager.enqueue_request(
                user_id=user_id,
                request_type="barcode_recommendation",
                request_data=validation_result.cleaned_data,
                priority=RequestPriority.NORMAL
            )
            workflow_steps.append({"step": "è¯·æ±‚å…¥é˜Ÿ", "status": "æˆåŠŸ", "request_id": request_id})
            
            # æ­¥éª¤6: ç­‰å¾…å¤„ç†å®Œæˆ
            await asyncio.sleep(0.5)
            
            # æ­¥éª¤7: è·å–ç»“æœ
            request_result = await queue_manager.get_request_status(request_id)
            processing_time = (time.time() - start_time) * 1000
            
            if request_result and request_result["status"] == "completed":
                workflow_steps.append({
                    "step": "è¯·æ±‚å¤„ç†", 
                    "status": "å®Œæˆ", 
                    "processing_time_ms": int(processing_time),
                    "has_recommendations": "recommendations" in request_result.get("result", {}),
                    "recommendation_count": len(request_result.get("result", {}).get("recommendations", []))
                })
            else:
                workflow_steps.append({"step": "è¯·æ±‚å¤„ç†", "status": "å¤±è´¥", "result": request_result})
                raise Exception("è¯·æ±‚å¤„ç†å¤±è´¥")
            
            # æ­¥éª¤8: æ›´æ–°ä¼šè¯æ´»åŠ¨
            session_manager.update_session_activity(
                user_id=user_id,
                request_type="barcode_recommendation",
                response_time_ms=int(processing_time),
                success=True
            )
            workflow_steps.append({"step": "ä¼šè¯æ›´æ–°", "status": "å®Œæˆ"})
            
            # æ­¥éª¤9: è·å–æœ€ç»ˆçŠ¶æ€
            session_summary = session_manager.get_session_summary(session_id)
            performance_report = get_performance_monitor().get_performance_report()
            
            await queue_manager.stop()
            
            # éªŒè¯å·¥ä½œæµç¨‹å®Œæ•´æ€§
            expected_steps = ["åˆ›å»ºç”¨æˆ·ä¼šè¯", "æ•°æ®éªŒè¯", "é‡å¤è¯·æ±‚æ£€æŸ¥", "è¯·æ±‚å…¥é˜Ÿ", "è¯·æ±‚å¤„ç†", "ä¼šè¯æ›´æ–°"]
            completed_steps = [step["step"] for step in workflow_steps if step["status"] in ["æˆåŠŸ", "å®Œæˆ", "é€šè¿‡"]]
            
            success = all(step in completed_steps for step in expected_steps)
            
            result.complete(success, {
                "workflow_steps": workflow_steps,
                "total_steps": len(workflow_steps),
                "successful_steps": len([s for s in workflow_steps if s["status"] in ["æˆåŠŸ", "å®Œæˆ", "é€šè¿‡"]]),
                "processing_time_ms": int(processing_time),
                "session_summary": {
                    "request_count": session_summary["request_count"],
                    "success_rate": session_summary["success_rate"]
                },
                "final_metrics": {
                    "total_requests": performance_report["request_metrics"]["total_requests"],
                    "ai_requests": performance_report["ai_metrics"]["total_ai_requests"],
                    "tokens_used": performance_report["ai_metrics"]["total_tokens"],
                    "estimated_cost": performance_report["ai_metrics"]["total_cost_usd"]
                }
            })
            
            if success:
                print(f"âœ… ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡ - {len(completed_steps)}/{len(expected_steps)}ä¸ªæ­¥éª¤å®Œæˆ")
            else:
                print(f"âŒ ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥")
                
        except Exception as e:
            result.complete(False, {"exception_type": type(e).__name__}, str(e))
            print(f"âŒ ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹æµ‹è¯•å¼‚å¸¸: {e}")
        
        return result
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹Grocery Guardian Recommendationæ¨¡å—ç”Ÿäº§ç¯å¢ƒå®Œæ•´æµ‹è¯•")
        print("=" * 80)
        print(f"æµ‹è¯•å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æµ‹è¯•ç¯å¢ƒ: {self.environment_info['environment']}")
        print("=" * 80)
        
        # å®šä¹‰æ‰€æœ‰æµ‹è¯•
        test_functions = [
            ("Azure OpenAIé›†æˆ", self.run_azure_openai_integration_test),
            ("å¤šç”¨æˆ·å¹¶å‘å¤„ç†", self.run_concurrent_processing_test),
            ("æ•°æ®éªŒè¯å’Œå®‰å…¨", self.run_data_validation_security_test),
            ("é”™è¯¯å¤„ç†å’Œæ¢å¤", self.run_error_recovery_test),
            ("æ€§èƒ½ç›‘æ§å’Œå¥åº·æ£€æŸ¥", self.run_performance_monitoring_test),
            ("ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹", self.run_end_to_end_workflow_test),
        ]
        
        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        for test_name, test_func in test_functions:
            print(f"\nğŸ“‹ æ‰§è¡Œæµ‹è¯•: {test_name}")
            print("-" * 60)
            
            try:
                test_result = await test_func()
                self.test_results.append(test_result)
                
                if test_result.success:
                    print(f"âœ… {test_name} - é€šè¿‡ (è€—æ—¶: {test_result.duration_ms}ms)")
                else:
                    print(f"âŒ {test_name} - å¤±è´¥ (è€—æ—¶: {test_result.duration_ms}ms)")
                    if test_result.error_message:
                        print(f"   é”™è¯¯: {test_result.error_message}")
                        
            except Exception as e:
                # åˆ›å»ºå¤±è´¥çš„æµ‹è¯•ç»“æœ
                failed_result = TestResult(test_name, "æœªçŸ¥")
                failed_result.complete(False, {"exception_type": type(e).__name__}, str(e))
                self.test_results.append(failed_result)
                print(f"âŒ {test_name} - å¼‚å¸¸: {e}")
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        self._calculate_test_summary()
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        report = self._generate_comprehensive_report()
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self._save_test_report(report)
        
        print("\n" + "=" * 80)
        print("ğŸ¯ æµ‹è¯•å®Œæˆæ€»ç»“")
        print("=" * 80)
        print(f"æ€»æµ‹è¯•æ•°: {self.test_summary['total_tests']}")
        print(f"é€šè¿‡: {self.test_summary['passed_tests']}")
        print(f"å¤±è´¥: {self.test_summary['failed_tests']}")
        print(f"æˆåŠŸç‡: {(self.test_summary['passed_tests']/self.test_summary['total_tests']*100):.1f}%")
        print(f"æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        if self.test_summary['failed_tests'] == 0:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Recommendationæ¨¡å—ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–å®Œæˆï¼Œå¯ä»¥æŠ•å…¥ä½¿ç”¨ã€‚")
            self.test_summary['system_status'] = "ready_for_production"
        else:
            print(f"âš ï¸ æœ‰ {self.test_summary['failed_tests']} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½ã€‚")
            self.test_summary['system_status'] = "needs_attention"
        
        return self.test_summary['failed_tests'] == 0
    
    def _calculate_test_summary(self):
        """è®¡ç®—æµ‹è¯•æ‘˜è¦"""
        self.test_summary['total_tests'] = len(self.test_results)
        self.test_summary['passed_tests'] = len([r for r in self.test_results if r.success])
        self.test_summary['failed_tests'] = len([r for r in self.test_results if not r.success])
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        for result in self.test_results:
            category = result.category
            if category not in self.test_summary['categories']:
                self.test_summary['categories'][category] = {'total': 0, 'passed': 0, 'failed': 0}
            
            self.test_summary['categories'][category]['total'] += 1
            if result.success:
                self.test_summary['categories'][category]['passed'] += 1
            else:
                self.test_summary['categories'][category]['failed'] += 1
        
        # æ€§èƒ½æŒ‡æ ‡æ±‡æ€»
        total_duration = sum([r.duration_ms for r in self.test_results])
        self.test_summary['performance_metrics'] = {
            'total_test_duration_ms': total_duration,
            'average_test_duration_ms': total_duration / len(self.test_results) if self.test_results else 0,
            'fastest_test_ms': min([r.duration_ms for r in self.test_results]) if self.test_results else 0,
            'slowest_test_ms': max([r.duration_ms for r in self.test_results]) if self.test_results else 0
        }
    
    def _generate_comprehensive_report(self) -> str:
        """ç”Ÿæˆå®Œæ•´çš„ä¸­æ–‡æµ‹è¯•æŠ¥å‘Š"""
        report_lines = []
        
        # æŠ¥å‘Šæ ‡é¢˜
        report_lines.extend([
            "# Grocery Guardian Recommendationæ¨¡å—ç”Ÿäº§ç¯å¢ƒæµ‹è¯•æŠ¥å‘Š",
            "",
            f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}",
            f"**æµ‹è¯•ç‰ˆæœ¬**: ProductionTestSuite v1.0",
            f"**æµ‹è¯•ç¯å¢ƒ**: {self.environment_info['environment']}",
            "",
            "## æ‰§è¡Œæ‘˜è¦",
            "",
            f"æœ¬æ¬¡æµ‹è¯•å¯¹Grocery Guardiané¡¹ç›®çš„Recommendationæ¨¡å—è¿›è¡Œäº†å…¨é¢çš„ç”Ÿäº§ç¯å¢ƒéªŒè¯ï¼Œæ¶µç›–äº†Azure OpenAIé›†æˆã€å¤šç”¨æˆ·å¹¶å‘å¤„ç†ã€æ•°æ®å®‰å…¨éªŒè¯ã€é”™è¯¯å¤„ç†æ¢å¤ã€æ€§èƒ½ç›‘æ§ä»¥åŠç«¯åˆ°ç«¯å·¥ä½œæµç¨‹ç­‰6ä¸ªæ ¸å¿ƒåŠŸèƒ½é¢†åŸŸã€‚",
            "",
            f"- **æ€»æµ‹è¯•æ•°**: {self.test_summary['total_tests']}é¡¹",
            f"- **é€šè¿‡æµ‹è¯•**: {self.test_summary['passed_tests']}é¡¹",
            f"- **å¤±è´¥æµ‹è¯•**: {self.test_summary['failed_tests']}é¡¹", 
            f"- **æˆåŠŸç‡**: {(self.test_summary['passed_tests']/self.test_summary['total_tests']*100):.1f}%",
            f"- **ç³»ç»ŸçŠ¶æ€**: {'âœ… ç”Ÿäº§å°±ç»ª' if self.test_summary['system_status'] == 'ready_for_production' else 'âš ï¸ éœ€è¦å…³æ³¨'}",
            "",
        ])
        
        # æµ‹è¯•ç¯å¢ƒä¿¡æ¯
        report_lines.extend([
            "## æµ‹è¯•ç¯å¢ƒä¿¡æ¯",
            "",
            f"- **Pythonç‰ˆæœ¬**: {self.environment_info['python_version'].split()[0]}",
            f"- **Azure OpenAIé…ç½®**: {'âœ… å·²é…ç½®' if self.environment_info['azure_openai_configured'] else 'âŒ æœªé…ç½®'}",
            f"- **OpenAIå¤‡ç”¨é…ç½®**: {'âœ… å·²é…ç½®' if self.environment_info['openai_fallback_configured'] else 'âŒ æœªé…ç½®'}",
            f"- **æµ‹è¯•å¼€å§‹æ—¶é—´**: {self.environment_info['test_start_time']}",
            "",
        ])
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        report_lines.extend([
            "## æµ‹è¯•ç±»åˆ«ç»Ÿè®¡",
            "",
            "| æµ‹è¯•ç±»åˆ« | æ€»æ•° | é€šè¿‡ | å¤±è´¥ | æˆåŠŸç‡ |",
            "|---------|------|------|------|--------|"
        ])
        
        for category, stats in self.test_summary['categories'].items():
            success_rate = (stats['passed'] / stats['total'] * 100) if stats['total'] > 0 else 0
            report_lines.append(f"| {category} | {stats['total']} | {stats['passed']} | {stats['failed']} | {success_rate:.1f}% |")
        
        report_lines.append("")
        
        # è¯¦ç»†æµ‹è¯•ç»“æœ
        report_lines.extend([
            "## è¯¦ç»†æµ‹è¯•ç»“æœ",
            ""
        ])
        
        for i, result in enumerate(self.test_results, 1):
            status_icon = "âœ…" if result.success else "âŒ"
            report_lines.extend([
                f"### {i}. {result.test_name} {status_icon}",
                "",
                f"- **æµ‹è¯•ç±»åˆ«**: {result.category}",
                f"- **æ‰§è¡Œæ—¶é—´**: {result.duration_ms}ms",
                f"- **æµ‹è¯•ç»“æœ**: {'é€šè¿‡' if result.success else 'å¤±è´¥'}",
            ])
            
            if result.error_message:
                report_lines.append(f"- **é”™è¯¯ä¿¡æ¯**: {result.error_message}")
            
            # æ·»åŠ è¯¦ç»†ç»“æœ
            if result.details:
                report_lines.append("- **è¯¦ç»†ä¿¡æ¯**:")
                for key, value in result.details.items():
                    if isinstance(value, dict):
                        report_lines.append(f"  - **{key}**:")
                        for sub_key, sub_value in value.items():
                            report_lines.append(f"    - {sub_key}: {sub_value}")
                    elif isinstance(value, list):
                        report_lines.append(f"  - **{key}**: {len(value)}é¡¹")
                        for item in value[:3]:  # åªæ˜¾ç¤ºå‰3é¡¹
                            if isinstance(item, dict):
                                item_summary = ", ".join([f"{k}: {v}" for k, v in list(item.items())[:2]])  # åªæ˜¾ç¤ºå‰2ä¸ªå­—æ®µ
                                report_lines.append(f"    - {item_summary}")
                            else:
                                report_lines.append(f"    - {item}")
                        if len(value) > 3:
                            report_lines.append(f"    - ... (è¿˜æœ‰{len(value)-3}é¡¹)")
                    else:
                        report_lines.append(f"  - **{key}**: {value}")
            
            report_lines.append("")
        
        # æ€§èƒ½æŒ‡æ ‡åˆ†æ
        report_lines.extend([
            "## æ€§èƒ½æŒ‡æ ‡åˆ†æ",
            "",
            f"- **æ€»æµ‹è¯•è€—æ—¶**: {self.test_summary['performance_metrics']['total_test_duration_ms']}ms ({self.test_summary['performance_metrics']['total_test_duration_ms']/1000:.1f}ç§’)",
            f"- **å¹³å‡æµ‹è¯•è€—æ—¶**: {self.test_summary['performance_metrics']['average_test_duration_ms']:.1f}ms",
            f"- **æœ€å¿«æµ‹è¯•**: {self.test_summary['performance_metrics']['fastest_test_ms']}ms",
            f"- **æœ€æ…¢æµ‹è¯•**: {self.test_summary['performance_metrics']['slowest_test_ms']}ms",
            "",
        ])
        
        # ç”Ÿäº§å°±ç»ªè¯„ä¼°
        report_lines.extend([
            "## ç”Ÿäº§å°±ç»ªè¯„ä¼°",
            "",
        ])
        
        if self.test_summary['system_status'] == 'ready_for_production':
            report_lines.extend([
                "### âœ… ç³»ç»Ÿç”Ÿäº§å°±ç»ª",
                "",
                "æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼Œç³»ç»Ÿå…·å¤‡ä»¥ä¸‹ç”Ÿäº§èƒ½åŠ›ï¼š",
                "",
                "1. **AIæœåŠ¡é›†æˆ**: Azure OpenAIé›†æˆæ­£å¸¸ï¼Œæ”¯æŒå®é™…çš„AIæ¨èç”Ÿæˆ",
                "2. **å¹¶å‘å¤„ç†**: æ”¯æŒå¤šç”¨æˆ·å¹¶å‘è®¿é—®ï¼Œé˜Ÿåˆ—ç®¡ç†å·¥ä½œæ­£å¸¸",
                "3. **æ•°æ®å®‰å…¨**: è¾“å…¥éªŒè¯å’Œå®‰å…¨è¿‡æ»¤æœºåˆ¶æœ‰æ•ˆé˜²æŠ¤",
                "4. **é”™è¯¯æ¢å¤**: å®Œå–„çš„é”™è¯¯å¤„ç†å’Œè‡ªåŠ¨é™çº§æœºåˆ¶",
                "5. **æ€§èƒ½ç›‘æ§**: å®æ—¶æ€§èƒ½æŒ‡æ ‡æ”¶é›†å’Œå¥åº·çŠ¶æ€ç›‘æ§",
                "6. **ç«¯åˆ°ç«¯æµç¨‹**: å®Œæ•´çš„è¯·æ±‚å¤„ç†å·¥ä½œæµç¨‹éªŒè¯é€šè¿‡",
                "",
                "**å»ºè®®**: ç³»ç»Ÿå¯ä»¥éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œå»ºè®®è¿›è¡Œè´Ÿè½½æµ‹è¯•ä»¥éªŒè¯é«˜å¹¶å‘æ€§èƒ½ã€‚",
                ""
            ])
        else:
            failed_tests = [r for r in self.test_results if not r.success]
            report_lines.extend([
                "### âš ï¸ ç³»ç»Ÿéœ€è¦å…³æ³¨",
                "",
                f"æœ‰{len(failed_tests)}ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤ä»¥ä¸‹é—®é¢˜ï¼š",
                ""
            ])
            
            for failed_test in failed_tests:
                report_lines.extend([
                    f"- **{failed_test.test_name}**: {failed_test.error_message}",
                ])
            
            report_lines.extend([
                "",
                "**å»ºè®®**: ä¿®å¤å¤±è´¥çš„æµ‹è¯•é¡¹ç›®åé‡æ–°è¿›è¡Œæµ‹è¯•éªŒè¯ã€‚",
                ""
            ])
        
        # æŠ€æœ¯æ¶æ„è¯´æ˜
        report_lines.extend([
            "## æŠ€æœ¯æ¶æ„è¯´æ˜",
            "",
            "### æ ¸å¿ƒç»„ä»¶",
            "",
            "1. **Azure OpenAIå®¢æˆ·ç«¯** (`llm_evaluation/azure_openai_client.py`)",
            "   - æä¾›Azure OpenAIæœåŠ¡é›†æˆ",
            "   - æ”¯æŒè‡ªåŠ¨é‡è¯•å’Œé”™è¯¯å¤„ç†",
            "   - å®æ—¶ä½¿ç”¨ç»Ÿè®¡å’Œæˆæœ¬è·Ÿè¸ª",
            "",
            "2. **è¯·æ±‚é˜Ÿåˆ—ç®¡ç†å™¨** (`common/request_queue.py`)",
            "   - å¼‚æ­¥è¯·æ±‚å¤„ç†å’Œå¹¶å‘æ§åˆ¶",
            "   - ç”¨æˆ·çº§åˆ«çš„é€Ÿç‡é™åˆ¶",
            "   - ä¼˜å…ˆçº§é˜Ÿåˆ—å’Œé‡å¤è¯·æ±‚æ£€æµ‹",
            "",
            "3. **æ•°æ®éªŒè¯å™¨** (`common/data_validator.py`)",
            "   - å…¨é¢çš„è¾“å…¥æ•°æ®éªŒè¯å’Œæ¸…ç†",
            "   - å®‰å…¨è¿‡æ»¤å’Œæ¶æ„è¾“å…¥æ£€æµ‹",
            "   - æ”¯æŒå¤šç§è¯·æ±‚ç±»å‹éªŒè¯",
            "",
            "4. **é”™è¯¯å¤„ç†å™¨** (`common/error_handler.py`)",
            "   - ç»Ÿä¸€çš„é”™è¯¯åˆ†ç±»å’Œå¤„ç†",
            "   - è‡ªåŠ¨é”™è¯¯æ¢å¤å’Œé‡è¯•æœºåˆ¶",
            "   - é”™è¯¯ç»Ÿè®¡å’Œæ¨¡å¼åˆ†æ",
            "",
            "5. **é™çº§æœåŠ¡** (`common/fallback_service.py`)",
            "   - åŸºäºè§„åˆ™çš„å¤‡ç”¨æ¨èæœåŠ¡",
            "   - AIæœåŠ¡ä¸å¯ç”¨æ—¶çš„è‡ªåŠ¨é™çº§",
            "   - ä¿è¯ç³»ç»ŸåŸºç¡€å¯ç”¨æ€§",
            "",
            "6. **æ€§èƒ½ç›‘æ§å™¨** (`monitoring/performance_monitor.py`)",
            "   - å®æ—¶æ€§èƒ½æŒ‡æ ‡æ”¶é›†",
            "   - ç³»ç»Ÿèµ„æºç›‘æ§å’Œå‘Šè­¦",
            "   - å†å²æ•°æ®å­˜å‚¨å’Œåˆ†æ",
            "",
            "7. **å¥åº·æ£€æŸ¥å™¨** (`monitoring/health_checker.py`)",
            "   - æœåŠ¡å¥åº·çŠ¶æ€ç›‘æ§",
            "   - è‡ªåŠ¨å¥åº·æ£€æŸ¥è°ƒåº¦",
            "   - ç³»ç»Ÿæ•´ä½“å¥åº·è¯„ä¼°",
            "",
        ])
        
        # éƒ¨ç½²å»ºè®®
        report_lines.extend([
            "## éƒ¨ç½²å»ºè®®",
            "",
            "### ç¯å¢ƒå˜é‡é…ç½®",
            "",
            "ç¡®ä¿ä»¥ä¸‹ç¯å¢ƒå˜é‡æ­£ç¡®é…ç½®ï¼š",
            "",
            "```bash",
            "# Azure OpenAIé…ç½®ï¼ˆä¸»è¦ï¼‰",
            "AZURE_OPENAI_API_KEY=your_azure_api_key",
            "AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/",
            "AZURE_OPENAI_API_VERSION=2024-02-01",
            "AZURE_OPENAI_MODEL=gpt-4o-mini-prod",
            "",
            "# OpenAIé…ç½®ï¼ˆå¤‡ç”¨ï¼‰",
            "OPENAI_API_KEY=your_openai_api_key",
            "",
            "# æ€§èƒ½é…ç½®",
            "MAX_CONCURRENT_REQUESTS=15",
            "MAX_QUEUE_SIZE=200",
            "REQUEST_TIMEOUT=45",
            "",
            "# ç›‘æ§é…ç½®",
            "MONITORING_ENABLED=true",
            "METRICS_COLLECTION_INTERVAL=60",
            "```",
            "",
            "### å®¹é‡è§„åˆ’",
            "",
            "- **å¹¶å‘ç”¨æˆ·**: å»ºè®®åˆå§‹é…ç½®æ”¯æŒ100+å¹¶å‘ç”¨æˆ·",
            "- **è¯·æ±‚å¤„ç†**: 15ä¸ªå¹¶å‘AIè¯·æ±‚ï¼Œé˜Ÿåˆ—å®¹é‡200",
            "- **å“åº”æ—¶é—´**: 95%è¯·æ±‚åœ¨3ç§’å†…å®Œæˆ",
            "- **æˆæœ¬æ§åˆ¶**: Azure OpenAIæŒ‰tokenè®¡è´¹ï¼Œå»ºè®®è®¾ç½®é¢„ç®—å‘Šè­¦",
            "",
        ])
        
        # ç›‘æ§å’Œç»´æŠ¤
        report_lines.extend([
            "## ç›‘æ§å’Œç»´æŠ¤",
            "",
            "### å…³é”®æŒ‡æ ‡ç›‘æ§",
            "",
            "1. **æ€§èƒ½æŒ‡æ ‡**",
            "   - è¯·æ±‚æˆåŠŸç‡ (ç›®æ ‡: >99%)",
            "   - å¹³å‡å“åº”æ—¶é—´ (ç›®æ ‡: <2ç§’)",
            "   - å¹¶å‘è¯·æ±‚æ•°é‡",
            "   - é˜Ÿåˆ—å¤§å°å’Œå¤„ç†é€Ÿåº¦",
            "",
            "2. **AIæœåŠ¡æŒ‡æ ‡**",
            "   - Tokenä½¿ç”¨é‡å’Œæˆæœ¬",
            "   - AIè¯·æ±‚æˆåŠŸç‡",
            "   - æ¨¡å‹å“åº”æ—¶é—´",
            "   - é™çº§æœåŠ¡ä½¿ç”¨é¢‘ç‡",
            "",
            "3. **ç³»ç»Ÿèµ„æº**",
            "   - CPUå’Œå†…å­˜ä½¿ç”¨ç‡",
            "   - ç½‘ç»œIOå’Œç£ç›˜ä½¿ç”¨",
            "   - æ•°æ®åº“è¿æ¥æ± çŠ¶æ€",
            "",
            "### æ—¥å¸¸ç»´æŠ¤ä»»åŠ¡",
            "",
            "- å®šæœŸæ£€æŸ¥é”™è¯¯æ—¥å¿—å’Œå‘Šè­¦",
            "- ç›‘æ§AIæœåŠ¡æˆæœ¬å’Œä½¿ç”¨é…é¢",
            "- æ›´æ–°å®‰å…¨è¿‡æ»¤è§„åˆ™",
            "- å¤‡ä»½æ€§èƒ½æŒ‡æ ‡å’Œé…ç½®",
            "",
        ])
        
        # æŠ¥å‘Šç»“å°¾
        report_lines.extend([
            "---",
            "",
            f"**æŠ¥å‘Šç”Ÿæˆ**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}",
            f"**æµ‹è¯•å·¥å…·**: Grocery Guardianç”Ÿäº§æµ‹è¯•å¥—ä»¶ v1.0",
            f"**è”ç³»ä¿¡æ¯**: å¦‚æœ‰é—®é¢˜è¯·è”ç³»å¼€å‘å›¢é˜Ÿ",
            ""
        ])
        
        return "\n".join(report_lines)
    
    def _save_test_report(self, report_content: str) -> str:
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"recommendation_production_test_report_{timestamp}.md"
        report_path = os.path.join(os.path.dirname(__file__), report_filename)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        return report_path

async def main():
    """ä¸»å‡½æ•°"""
    # è‡ªåŠ¨è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆä»Dockeré…ç½®ä¸­è·å–ï¼‰
    if not os.getenv("AZURE_OPENAI_API_KEY"):
        os.environ["AZURE_OPENAI_API_KEY"] = "73540b77e3304ee8b9614e8593f08f02"
    if not os.getenv("AZURE_OPENAI_ENDPOINT"):
        os.environ["AZURE_OPENAI_ENDPOINT"] = "https://xiangopenai2025.openai.azure.com/"
    if not os.getenv("AZURE_OPENAI_API_VERSION"):
        os.environ["AZURE_OPENAI_API_VERSION"] = "2024-02-01"
    if not os.getenv("AZURE_OPENAI_MODEL"):
        os.environ["AZURE_OPENAI_MODEL"] = "gpt-4o-mini-prod"
    if not os.getenv("OPENAI_API_KEY"):
        os.environ["OPENAI_API_KEY"] = "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"
    if not os.getenv("ENVIRONMENT"):
        os.environ["ENVIRONMENT"] = "java_integration"
    
    print("ğŸ”§ ç¯å¢ƒå˜é‡å·²è‡ªåŠ¨é…ç½®")
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶å¹¶è¿è¡Œ
    test_suite = ProductionTestSuite()
    success = await test_suite.run_all_tests()
    
    return success

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)